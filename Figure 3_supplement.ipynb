{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "from glob import glob as glob\n",
    "import os\n",
    "from neo import io\n",
    "import seaborn as sb\n",
    "from sklearn import preprocessing\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "plt.rcParams[\"font.size\"]= 12\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed data and extract sequence variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PFC_MU', 'PPC_MU', 'PFC_SU', 'PPC_SU', 'TrialID', 'ItemID', 'StimID', 'StimName', 'StimOn', 'blockID', 'blockType', 'date', 'StimDur', 'ISIDur', 'RewardOn']\n",
      "Recording dates: ['20200226' '20200228' '20200305' '20200306' '20200311' '20200312']\n",
      "Day: 20200226\n",
      "Day: 20200228\n",
      "Day: 20200305\n",
      "Day: 20200306\n",
      "Day: 20200311\n",
      "Day: 20200312\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "animal_id = 'A11' # A11, H07\n",
    "exp = 'classical' # 'wang or classical\n",
    "out_path = '/Volumes/Bellet/PhD/Local_Global/processedData/%s/'%animal_id # path to the data\n",
    "\n",
    "# load preprocessed data\n",
    "df = pd.read_pickle(os.path.join(out_path,'%s_stims_spikes_dataframe.pkl'%(animal_id)))\n",
    "\n",
    "# extract basic variables\n",
    "dates = np.unique(df.date)\n",
    "ndates = len(dates)\n",
    "nitems = np.max(df.ItemID)\n",
    "stimID = np.unique(df.StimID)\n",
    "nstim = len(stimID)\n",
    "print(list(df.keys()))\n",
    "print('Recording dates:',dates)\n",
    "\n",
    "soa = 600\n",
    "Local = []\n",
    "Global = []\n",
    "BlockID = []\n",
    "Blocktype = []\n",
    "Block = [] # which block\n",
    "Include = []\n",
    "Stimon = []\n",
    "SessionDate = []\n",
    "StimID = []\n",
    "sess = 0\n",
    "blocks = ['aa','aB','bb','bA'] # coded like this in the experiment\n",
    "for d,date in enumerate(dates):\n",
    "\n",
    "\n",
    "    \n",
    "    # get conditions\n",
    "    last_item = np.where((df.ItemID==3) & (df.date==date) )[0]#& ((df.StimDur+df.ISIDur)==soa))[0]\n",
    "    Ntrials = len(last_item)\n",
    "    if Ntrials>0:\n",
    "        \n",
    "        print('Day:',date)\n",
    "        SessionDate.append(date)\n",
    "        Local.append(np.zeros(Ntrials).astype(int))\n",
    "        for i,ind in enumerate(last_item):\n",
    "            if df.StimID.iloc[ind]!=df.StimID.iloc[ind-1]: # local deviant = transition of two stim\n",
    "                Local[sess][i] = 1\n",
    "        \n",
    "        BlockID.append(np.array(df.blockID[last_item])) # blockID\n",
    "        Block.append(np.array(df.blockType[last_item])) # which block (classical experiment: 0,1,2 or 3)\n",
    "        Blocktype.append(np.zeros(Ntrials).astype(int)) # xx=0 xY=1\n",
    "        for bl in np.unique(BlockID[sess]):\n",
    "            ind = np.where(BlockID[sess]==bl)[0]\n",
    "            Blocktype[sess][ind] = Local[sess][ind[0]] # blocktype defined by the first habituation trial in block\n",
    "        \n",
    "        Global.append((Local[sess]!=Blocktype[sess]).astype(int))\n",
    "        \n",
    "        # filter habituation trials\n",
    "        nhab = 50\n",
    "        Include.append(np.ones(len(Global[sess])).astype(int))\n",
    "        for bl in np.unique(BlockID[sess]):\n",
    "            ind = np.where(BlockID[sess]==bl)[0]\n",
    "            Include[sess][ind[:nhab]] = 0\n",
    "            \n",
    "        Stimon.append(np.array(df.StimOn.iloc[last_item])) # last stim onset times\n",
    "        StimID.append(np.array(df.StimID.iloc[last_item]))\n",
    "\n",
    "        \n",
    "        sess += 1\n",
    "    \n",
    "Session = np.concatenate([np.ones(len(Local[i]))*i for i in range(len(Local))])\n",
    "Local = np.concatenate(Local)\n",
    "Global = np.concatenate(Global)\n",
    "Blocktype = np.concatenate(Blocktype)\n",
    "Block = np.concatenate(Block)\n",
    "Include = np.concatenate(Include)\n",
    "StimID = np.concatenate(StimID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load PSTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day: 20200226\n",
      "Day: 20200228\n",
      "Day: 20200305\n",
      "Day: 20200306\n",
      "Day: 20200311\n",
      "Day: 20200312\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load PSTH\n",
    "MUA = []\n",
    "sess = 0\n",
    "for d,date in enumerate(dates):\n",
    "    # get conditions\n",
    "    last_item = np.where((df.ItemID==3) & (df.date==date) )[0]#& ((df.StimDur+df.ISIDur)==soa))[0]\n",
    "    Ntrials = len(last_item)\n",
    "    if Ntrials>0:\n",
    "        \n",
    "        print('Day:',date)\n",
    "        \n",
    "        # whole sequence\n",
    "        Rb = np.load(os.path.join(out_path,'Rb_seq_%s.npy'%date))\n",
    "        \n",
    "        ntr,nch,nbins = Rb.shape\n",
    "        \n",
    "        MUA.append(Rb)\n",
    "        \n",
    "        sess += 1\n",
    "        \n",
    "del Rb\n",
    "\n",
    "MUA = np.concatenate(MUA)\n",
    "time_bins = np.load(os.path.join(out_path,'time_bins_seq_%s.npy'%dates[0])) # time vector\n",
    "binsize = np.round(np.mean(np.diff(time_bins)),3)\n",
    "time_bins = time_bins + binsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responsiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use t-test to get p-value\n",
    "from scipy.stats import ttest_ind, ttest_rel, ranksums\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "alpha = 0.01 # significance thrreshold\n",
    "\n",
    "soa = 0.6 # soa of data\n",
    "#### TIME WINDOWS ####\n",
    "window_fix = np.arange(0, np.argmin(abs(time_bins)))\n",
    "window_seq = np.arange( int((abs(time_bins[0])+3*soa)/binsize), int((abs(time_bins[0])*2 + 3*soa)/binsize)) # define averaging window to average firing rate\n",
    "\n",
    "nsess = len(dates); nch = MUA.shape[1]\n",
    "T_seq = np.zeros((nsess,nch))\n",
    "P_seq = np.zeros((nsess,nch))\n",
    "for sess in range(nsess):\n",
    "    ind = Session==sess\n",
    "    for ch in range(nch):\n",
    "        fix_resp =  np.mean(MUA[ind,ch,:][:,window_fix],-1) # average response to stimulus A\n",
    "        seq_resp =  np.mean(MUA[ind,ch,:][:,window_seq],-1) # average response to stimulus A\n",
    "        T_seq[sess,ch], P_seq[sess,ch] = ttest_rel(fix_resp,seq_resp)\n",
    "\n",
    "# run false discovery rate procedure on p values from all sessions and channels together\n",
    "p_flat = P_seq.flatten()\n",
    "fdr = multipletests(p_flat, alpha=alpha)\n",
    "p_corr = fdr[1]\n",
    "P_seq = np.reshape(p_corr,(nsess,nch)) # put back into original matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC\n",
    "Determine bin with maximum performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_stim = np.load(os.path.join(out_path,'AUC_Stim_first3_classical_600.npy'))\n",
    "AUC_ctx = np.load(os.path.join(out_path,'AUC_Block_first3_classical_600.npy'))\n",
    "AUC_loc = np.load(os.path.join(out_path,'AUC_Local_last_True_classical_600.npy'))\n",
    "AUC_glob = np.load(os.path.join(out_path,'AUC_Global_last_True_classical_600.npy'))\n",
    "maxbin_stim = np.argmax(np.mean(np.diagonal(AUC_stim[:,:,0,:]),1))\n",
    "maxbin_ctx = np.argmax(np.mean(np.diagonal(AUC_ctx),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load coefficients\n",
    "Coef = np.load(os.path.join(out_path,'Coef_last_classical_600.npy'))\n",
    "RandCoef = np.load(os.path.join(out_path,'CoefRand_last_classical_600.npy'))\n",
    "Coef_seq = np.load(os.path.join(out_path,'Coef_first3_classical_600.npy'))\n",
    "RandCoef_seq = np.load(os.path.join(out_path,'CoefRand_first3_classical_600.npy'))\n",
    "ItemCoef = np.load(os.path.join(out_path,'ItemNumber_Coef.npy'))\n",
    "RandItemCoef = np.load(os.path.join(out_path,'ItemNumber_CoefRand.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recanatesi et al. Participation Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nperm = RandCoef.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_ax = 0\n",
    "glob_ax = 1\n",
    "stim_ax = 0\n",
    "ctx_ax = 1\n",
    "\n",
    "PRloc = np.zeros(nsess)\n",
    "PRglob = np.zeros(nsess)\n",
    "PRstim = np.zeros(nsess)\n",
    "PRctx = np.zeros(nsess)\n",
    "PRitem = np.zeros((4,nsess))\n",
    "\n",
    "PRperm = np.zeros((nperm,nsess))\n",
    "\n",
    "PRsparse = np.zeros(nsess)\n",
    "PRdist = np.zeros(nsess)\n",
    "\n",
    "T = Coef.shape[-1] # Number of repeats\n",
    "\n",
    "for sess in range(nsess):\n",
    "    # get bin of maximum decoding performance\n",
    "    max_loc = np.argmax(np.diagonal(AUC_loc[sess,sess,0,]))\n",
    "    max_glob = np.argmax(np.diagonal(AUC_loc[sess,sess,0,]))\n",
    "    \n",
    "    includeChannels = P_seq[sess,:]<alpha\n",
    "    \n",
    "    # PR for local axis\n",
    "    M = Coef[sess,loc_ax,includeChannels,max_loc,:].T\n",
    "    E = np.cov(M.T)\n",
    "    PRloc[sess] = np.trace(E)**2 / np.trace(E**2)\n",
    "    \n",
    "    # PR for global axis\n",
    "    M = Coef[sess,glob_ax,includeChannels,max_glob,:].T\n",
    "    E = np.cov(M.T)\n",
    "    PRglob[sess] = np.trace(E)**2 / np.trace(E**2)\n",
    "    \n",
    "    # PR for stimulus axis\n",
    "    M = Coef_seq[sess,stim_ax,includeChannels,:].T\n",
    "    E = np.cov(M.T)\n",
    "    PRstim[sess] = np.trace(E)**2 / np.trace(E**2)\n",
    "    \n",
    "    # PR for context axis\n",
    "    M = Coef_seq[sess,ctx_ax,includeChannels,:].T\n",
    "    E = np.cov(M.T)\n",
    "    PRctx[sess] = np.trace(E)**2 / np.trace(E**2)\n",
    "    \n",
    "    # PR for item number axis\n",
    "    for k in range(4):\n",
    "        M = ItemCoef[sess,k,includeChannels,:].T\n",
    "        E = np.cov(M.T)\n",
    "        PRitem[k,sess] = np.trace(E)**2 / np.trace(E**2)\n",
    "    \n",
    "    \n",
    "    # simulation:\n",
    "    \n",
    "    N = sum(includeChannels==True)\n",
    "    units = 1\n",
    "    idx = np.random.randint(0,N,units)\n",
    "    M = np.zeros((T,N)) #M = np.zeros((T,N))\n",
    "    M[:,idx] = np.repeat(np.random.rand(T,1),units,1) #M [:,idx] = np.random.rand(T,units)\n",
    "    #M = np.repeat(M,T,0)\n",
    "    \n",
    "    E = np.cov(M.T)\n",
    "    \n",
    "    PRsparse[sess] = np.trace(E)**2 / np.trace(E**2)\n",
    "    \n",
    "    units = N\n",
    "    M = np.zeros((T,N)) #M = np.zeros((T,N))\n",
    "    M = np.repeat(np.random.rand(T,1),units,1) #M [:,idx] = np.random.rand(T,units)\n",
    "    #M = np.repeat(M,T,0)\n",
    "    \n",
    "    E = np.cov(M.T)\n",
    "    \n",
    "    PRdist[sess] = np.trace(E)**2 / np.trace(E**2)\n",
    "    \n",
    "    # randperm\n",
    "    for k in range(nperm):   \n",
    "        # local\n",
    "        M = RandCoef[k,sess,glob_ax,includeChannels,max_glob,:].T\n",
    "        E = np.cov(M.T)\n",
    "        PRperm[k,sess] = np.trace(E)**2 / np.trace(E**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = PRglob/np.mean(PRperm,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR for local axis: 0.7174884234700447\n",
      "PR for global axis: 0.7350869743165475\n",
      "PR for stimulus axis: 0.7295121181248808\n",
      "PR for context axis: 0.7204500657286976\n",
      "PR for item 1: 0.7435010134427434\n",
      "PR for item 2: 0.7240906616304336\n",
      "PR for item 3: 0.7421479372756061\n",
      "PR for item 4: 0.7156472452726833\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ratio = PRloc/PRdist\n",
    "print('PR for local axis:',np.mean(ratio))\n",
    "ratio = PRglob/PRdist\n",
    "print('PR for global axis:',np.mean(ratio))\n",
    "ratio = PRstim/PRdist\n",
    "print('PR for stimulus axis:',np.mean(ratio))\n",
    "ratio = PRctx/PRdist\n",
    "print('PR for context axis:',np.mean(ratio))\n",
    "for k in range(4):\n",
    "    ratio = PRitem[k,:]/PRdist\n",
    "    print('PR for item %s:'%(k+1),np.mean(ratio))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
